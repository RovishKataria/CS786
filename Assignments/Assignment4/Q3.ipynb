{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MTech Courses\\CS786 Cognitive\\Assignments\\Assignment 4\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.read_csv(\"X.csv\", names=[\"weight\", \"height\", \"label\"])\n",
    "y_df = pd.read_csv(\"y.csv\", names=[\"weight\", 'height'])\n",
    "\n",
    "X1_df, X2_df = np.array_split(X_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X.csv', header=None).values\n",
    "y_test = pd.read_csv('y.csv', header=None).values\n",
    "\n",
    "# Separate data into weight, height, and labels\n",
    "weights = X[:, 0]\n",
    "heights = X[:, 1]\n",
    "\n",
    "# Categories: 1 = small, 2 = average, 3 = large\n",
    "labels = X[:, 2]\n",
    "\n",
    "# Split data by category\n",
    "small = X[labels == 1]\n",
    "average = X[labels == 2]\n",
    "large = X[labels == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean and standard deviation of weight and height for each category.\n",
    "params = {\n",
    "    'small': {\n",
    "        'weight_mean': np.mean(small[:, 0]),\n",
    "        'weight_std': np.std(small[:, 0]),\n",
    "        'height_mean': np.mean(small[:, 1]),\n",
    "        'height_std': np.std(small[:, 1])\n",
    "    },\n",
    "    'average': {\n",
    "        'weight_mean': np.mean(average[:, 0]),\n",
    "        'weight_std': np.std(average[:, 0]),\n",
    "        'height_mean': np.mean(average[:, 1]),\n",
    "        'height_std': np.std(average[:, 1])\n",
    "    },\n",
    "    'large': {\n",
    "        'weight_mean': np.mean(large[:, 0]),\n",
    "        'weight_std': np.std(large[:, 0]),\n",
    "        'height_mean': np.mean(large[:, 1]),\n",
    "        'height_std': np.std(large[:, 1])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(test_point, train_features, train_labels, alpha_weight=2, alpha_height=1, beta=1):\n",
    "    # Define attention weights for each training point\n",
    "    alpha = np.array([alpha_weight, alpha_height])\n",
    "    \n",
    "    # Calculating similarity for each training point\n",
    "    similarities = []\n",
    "    for i, exemplar in enumerate(train_features):\n",
    "        # Computing distance\n",
    "        distance = np.sum(alpha * np.abs(exemplar - test_point))\n",
    "        \n",
    "        # Computing similarity\n",
    "        similarity = np.exp(-beta * distance)\n",
    "        \n",
    "        # Appending similarity along with its label\n",
    "        similarities.append((similarity, train_labels[i]))\n",
    "\n",
    "    # Aggregating similarities by category\n",
    "    small_sim = sum(similar for similar, label in similarities if label == 1)\n",
    "    average_sim = sum(similar for similar, label in similarities if label == 2)\n",
    "    large_sim = sum(similar for similar, label in similarities if label == 3)\n",
    "\n",
    "    # Apply politeness bias: reduce the similarity weight for \"large\"\n",
    "    large_sim *= 0.8  # This factor represents the politeness adjustment\n",
    "\n",
    "    # Choose category with highest similarity score\n",
    "    similarities_dict = {1: small_sim, 2: average_sim, 3: large_sim}\n",
    "    return max(similarities_dict, key=similarities_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(weight, height, category, params):\n",
    "    \"\"\"Calculate likelihood for weight and height using Gaussian distributions\"\"\"\n",
    "    weight_likelihood = norm.pdf(weight, params[category]['weight_mean'], params[category]['weight_std'])\n",
    "    height_likelihood = norm.pdf(height, params[category]['height_mean'], params[category]['height_std'])\n",
    "    \n",
    "    # Adjust likelihood based on weight's higher importance \n",
    "    likelihood = (weight_likelihood ** 0.7) * (height_likelihood ** 0.3)\n",
    "    return likelihood\n",
    "\n",
    "def posterior_prob(weight, height, params):\n",
    "    \"\"\"Calculate posterior probability for each category\"\"\"\n",
    "    posterior_probs = {}\n",
    "    categories = ['small', 'average', 'large']\n",
    "    cat_labels = {category: i + 1 for i, category in enumerate(categories)}\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_label = cat_labels[category]\n",
    "        posterior_probs[cat_label] = likelihood(weight, height, category, params)\n",
    "    \n",
    "    # Normalize the probabilities so that they sum to 1\n",
    "    total_posterior = sum(posterior_probs.values())\n",
    "    for cat_label in posterior_probs:\n",
    "        posterior_probs[cat_label] /= total_posterior\n",
    "    \n",
    "    return posterior_probs\n",
    "\n",
    "def predict_cat_label(posterior_probs):\n",
    "    \"\"\"Find the category with the highest posterior probability\"\"\"\n",
    "    return max(posterior_probs, key=posterior_probs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcm_train_predict(X_train, y_test):\n",
    "    predictions = []\n",
    "    for test_point in y_test.values:\n",
    "        similarity = calculate_similarity(test_point, X_train.iloc[:, :2].values, X_train['label'].values)\n",
    "        predictions.append(similarity)\n",
    "    return predictions\n",
    "\n",
    "GCM_X1_predictions = gcm_train_predict(X1_df, y_df)\n",
    "GCM_X2_predictions = gcm_train_predict(X2_df, y_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmc_train_predict(X_train, y_test):\n",
    "    predictions = []\n",
    "    for test_point in y_test.values:\n",
    "        posterior_probs = posterior_prob(test_point[0], test_point[1], params)\n",
    "        predicted_label = predict_cat_label(posterior_probs)\n",
    "        predictions.append(predicted_label)\n",
    "    return predictions\n",
    "\n",
    "RMC_X1_predictions = rmc_train_predict(X1_df, y_df)\n",
    "RMC_X2_predictions = rmc_train_predict(X2_df, y_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCM Predictions:\n",
      "False\n",
      "RMC Predictions:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"GCM Predictions:\")\n",
    "print(np.array_equal(GCM_X1_predictions, GCM_X2_predictions))\n",
    "\n",
    "print(\"RMC Predictions:\")\n",
    "print(np.array_equal(RMC_X1_predictions, RMC_X2_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCM Predictions: Not Exchangeable\n",
    "The GCM model's predictions differ when trained on X1.csv versus X2.csv, indicating it does not treat the data as exchangeable.\n",
    "\n",
    "### Factors Contributing to GCM's Non-Exchangeability\n",
    "1. Similarity Calculation: The GCM model calculates similarity based on comparisons between each test point and every training point, making its predictions sensitive to the order and specifics of the data.\n",
    "2. Politeness Bias: A politeness bias factor (0.8) is applied to the \"large\" category, which could magnify minor variations in similarity calculations and further affect the model's consistency across datasets.\n",
    "\n",
    "### RMC Predictions: Exchangeable\n",
    "Unlike GCM, the RMC model’s predictions remain consistent across both X1.csv and X2.csv, demonstrating that it assumes exchangeability of data.\n",
    "\n",
    "### Reasons Behind RMC's Exchangeability\n",
    "1. Parametric Approach: RMC is built on a parametric model using Gaussian distributions for each category, making it more robust to variations in data order.\n",
    "2. Sufficient Statistics: The RMC model relies on summary statistics—mean and standard deviation—allowing it to capture the essential characteristics of the training data, thereby reducing sensitivity to the specific dataset arrangement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
